<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Uncategorized &#8211; Values in Design</title>
	<atom:link href="https://valuesindesign.wordpress.com/category/uncategorized/feed/" rel="self" type="application/rss+xml" />
	<link>https://valuesindesign.wordpress.com</link>
	<description></description>
	<lastBuildDate>Tue, 03 Sep 2019 18:22:23 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='valuesindesign.wordpress.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://secure.gravatar.com/blavatar/4a390b6eb04248a9744924c2c4cdabf1?s=96&#038;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</url>
		<title>Uncategorized &#8211; Values in Design</title>
		<link>https://valuesindesign.wordpress.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://valuesindesign.wordpress.com/osd.xml" title="Values in Design" />
	<atom:link rel='hub' href='https://valuesindesign.wordpress.com/?pushpress=hub'/>
	<item>
		<title>VALUES AT PLAY IN DIGITAL TECHNOLOGIES</title>
		<link>https://valuesindesign.wordpress.com/2019/09/03/values-at-play-in-digital-technologies/</link>
				<comments>https://valuesindesign.wordpress.com/2019/09/03/values-at-play-in-digital-technologies/#respond</comments>
				<pubDate>Tue, 03 Sep 2019 18:19:52 +0000</pubDate>
		<dc:creator><![CDATA[Sally Chen]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://valuesindesign.wordpress.com/?p=515</guid>
				<description><![CDATA[VALUES AT PLAY IN DIGITAL TECHNOLOGIES (Cornell Tech) Helen Nissenbaum (Professor) &#160; ABOUT THE COURSE Digital technologies have raised important...]]></description>
								<content:encoded><![CDATA[<p><strong>VALUES AT PLAY IN DIGITAL TECHNOLOGIES (Cornell Tech)</strong></p>
<p><strong>Helen Nissenbaum (Professor)</strong></p>
<p>&nbsp;</p>
<p><strong>ABOUT THE COURSE</strong></p>
<p>Digital technologies have raised important ethical questions. Although often the focus is how ethical issues arise from ways people have used these technologies, there is growing interest in ethical values associated with the design and development of digital systems and devices. The study of values in, or values embodied in technology, has grown in importance as engineers and computer, and data scientists have acknowledged that sound ethical practice is a responsibility alongside technical. This module introduces students to the latter. After providing a background to this way of thinking &#8212; about ethics as a dimension of technology design and development â€“ it introduces students to Values at Play, one particular approach to putting these ideas into practice. Students are encouraged to apply this learning to their own projects.</p>
<p><strong>INTENDED LEARNING OUTCOMES</strong></p>
<p>Students who successfully complete this course will be able to:</p>
<ul>
<li>Recognize how and to what extent values are implicated in technical artifacts, incidentally or by design.</li>
<li>Engage critically with everyday technical systems.<br />
Recognize instances of design that seem to elevate or obstruct certain values.</li>
<li>Engage actively with values embodied in particular systems or devices so as to recognized alternative designs with differing values implications.</li>
<li>Engage with fundamental concepts in the philosophy and social study of technology.</li>
<li>Critically analyze key social and political issues surrounding contemporary digital information systems and networks, e.g. privacy, intellectual property, freedom of speech.</li>
<li>Demonstrate conceptually or by prototype the values implications of particular design choices in particular systems.</li>
<li>An ability to think rigorously and systematically about values relevant to technical systems and features of systems.</li>
</ul>
<p>&nbsp;</p>
<p><strong>TEACHING AND LEARNING METHODOLOGIES</strong></p>
<p>Classes will comprise a variety of activity including instructor presentation, group discussion of readings, and individual and small group presentation of mini-projects. Instructor and group discussions will focus on concepts and arguments drawn from weekly reading assignments. After the first few foundation-setting class sessions, a segment of each class will be devoted to planning group projects: assembling students into collaborative pairs or threesomes, selecting topics, honing project goals, etc.</p>
<p>&nbsp;</p>
<p><strong>READING</strong></p>
<p>Course readings are essential to the class. It is crucial for all students to complete reading assignments before class meetings. I strongly encourage written notes annotated with page numbers, both to engage in discussion and, later, as sources written work. Students will be asked to post readings&#8217; responses to the course website. Readings vary considerably in discipline and level of difficulty; how challenging they are to students will depend on respective background familiarity with concepts, theories, and arguments. How much students benefit and learn from this course will depend heavily on how much efforts you invest in grappling and mastering the readings.</p>
<p><strong> </strong></p>
<p><strong>GRADING ELEMENTS</strong><br />
Participation (both in-class and posting on Blackboard discussion): 50%<br />
Project (design modifications inspired by values + ~2,000 word write-up): 50%</p>
<p>Students are expected to adhere to the Cornell Code of Academic Integrity.</p>
<p>&nbsp;</p>
<table>
<tbody>
<tr>
<td><strong>SCHEDULE</strong></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Mar 6</strong></td>
<td><strong>The idea of Values in Design</strong></td>
</tr>
<tr>
<td></td>
<td>Ethics and politics in technology</td>
</tr>
<tr>
<td></td>
<td>The Practical Turn</td>
</tr>
<tr>
<td></td>
<td>Being a conscientious designer</td>
</tr>
<tr>
<td></td>
<td>What values; whose values?</td>
</tr>
<tr>
<td><strong>Mar 13</strong></td>
<td><strong>VAP Framework &#8211; Discovery</strong></td>
</tr>
<tr>
<td></td>
<td>Sources of Values</td>
</tr>
<tr>
<td></td>
<td>Operational definition</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td><em>Values at Play</em>, chapters 1, 4-5</td>
</tr>
<tr>
<td></td>
<td>Berlin, Crooked Timber</td>
</tr>
<tr>
<td></td>
<td>Friedman and Nissenbaum, Bias</td>
</tr>
<tr>
<td></td>
<td>(For your personal edification, read articles by Latour)</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Mar 20</strong></td>
<td><strong>VAP Framework &#8211; Implementation</strong></td>
</tr>
<tr>
<td></td>
<td>Translation</td>
</tr>
<tr>
<td></td>
<td>Resolving Conflict</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Mar 27</strong></td>
<td><strong>VAP Framework &#8211; Verification</strong></td>
</tr>
<tr>
<td></td>
<td>Behavior, comprehension, attitude</td>
</tr>
<tr>
<td></td>
<td>Methods</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Module III</strong></td>
<td><strong>Private tutorials with small groups, 1-2 hours, to discuss individual projects. Tutorials will occur on April 10, 17, and 24th. Schedule TBA.</strong></td>
</tr>
</tbody>
</table>
]]></content:encoded>
							<wfw:commentRss>https://valuesindesign.wordpress.com/2019/09/03/values-at-play-in-digital-technologies/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:thumbnail url="https://valuesindesign.files.wordpress.com/2014/07/img_0243.jpg" />
		<media:content url="https://valuesindesign.files.wordpress.com/2014/07/img_0243.jpg" medium="image">
			<media:title type="html">IMG_0243</media:title>
		</media:content>

		<media:content url="https://2.gravatar.com/avatar/84b26454cc393776e28d4af11aadc1e4?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">cqx931</media:title>
		</media:content>
	</item>
		<item>
		<title>ETHICAL THINKING ABOUT DIGITAL TECHNOLOGIES AND DATA</title>
		<link>https://valuesindesign.wordpress.com/2019/08/19/ethical-thinking-about-digital-technologies-and-data/</link>
				<comments>https://valuesindesign.wordpress.com/2019/08/19/ethical-thinking-about-digital-technologies-and-data/#respond</comments>
				<pubDate>Mon, 19 Aug 2019 18:21:19 +0000</pubDate>
		<dc:creator><![CDATA[Sally Chen]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://valuesindesign.wordpress.com/?p=517</guid>
				<description><![CDATA[ETHICAL THINKING ABOUT DIGITAL TECHNOLOGIES AND DATA (Cornell Tech) Helen Nissenbaum (Professor) &#160; ABOUT THE COURSE Digital technologies, including computing,...]]></description>
								<content:encoded><![CDATA[<p><strong>ETHICAL THINKING ABOUT DIGITAL TECHNOLOGIES AND DATA (Cornell Tech) </strong></p>
<p><strong>Helen Nissenbaum (Professor)</strong></p>
<p>&nbsp;</p>
<p><strong>ABOUT THE COURSE</strong></p>
<p>Digital technologies, including computing, digital media, and data science are integrated into all aspects of contemporary life, from commerce, finance, education, politics, and entertainment to communication, transportation, and social life. They perform key functions and have made great positive contributions to quality of life. This course focuses on the ethical and political these technologies have raised; it studies them through the lens of social, political, and ethical values investigating whether and how technical systems promote or impede values to which we, individually and as societies, are committed &#8212; values, such as liberty, privacy, autonomy, and justice. At the same time as the course introduces students to key technologies, ethical concepts and diverse literatures, you will work individually and in groups, applying what you learn to real and hypothetical cases.</p>
<p>Taken together, the reading, writing, and discussing we do inside and outside the classroom will sharpen students&#8217; abilities to reason about ethical issues and social policy, and to grasp some of the deep and subtle connections between the design and performance characteristics of devices and systems, on the one hand, and ethical and political values, on the other. The course places special emphasis on ethical issues arising in the wake of powerful technologies for extracting, amassing, and analyzing data (&#8220;big data&#8221;), including data mining, machine learning, algorithmic decision-making and control, and AI.</p>
<p>The nature of the course subject matter requires the integration of multidisciplinary material and perspectives, reflected in wide-ranging readings, homework assignments, and discussions. Nevertheless consideration will be given to students&#8217; disciplinary and methodological strengths and skills. The course welcomes students with varied backgrounds and skills with prior understanding of, and experience with either computing (e.g. programming, website creation, active blogging, etc.) or social, political, and ethical analysis is recommended. It assumes that students are deeply interested in ethical and political issues relating to digital technologies and digital media as they affect individual lives and societies &#8211; issues, such as privacy, intellectual property, freedom of speech, and social justice.</p>
<p><strong>PREREQUISITES:</strong> none.<br />
<strong>READINGS:</strong> Weekly reading assignments will be posted on Course Blackboard.<br />
<strong>GRADING ELEMENTS*:</strong></p>
<p>Participation in class and on discussion board: 20%<br />
Weekly readings comments: 20%<br />
In-class presentation TBD: 10%<br />
Homework assignments: 50%</p>
<p>*A passing grade is needed for each element of the course. Missed deadlines will result in grade deductions at Instructors&#8217; discretion.</p>
<p><strong>INTENDED LEARNING OUTCOMES</strong></p>
<p>Students who successfully complete this course will be able to:</p>
<p>&#8211; Recognize and describe ethical issues relating to digital technologies.<br />
&#8211; Recognize how and to what extent values are implicated in technical artifacts, incidentally or by design.<br />
&#8211; Engage critically with everyday technical systems.<br />
&#8211; Recognize instances of design that seem to elevate or obstruct certain values.<br />
&#8211; Engage actively with values embodied in particular systems or devices so as to recognized alternative designs with differing values implications.<br />
&#8211; Engage with fundamental concepts in the philosophy and social study of technology.<br />
&#8211; Engage with fundamental concepts in ethical and political theory.<br />
&#8211; Critically analyze key social and political issues surrounding contemporary digital information systems and networks, e.g. privacy, intellectual property, freedom of speech.<br />
&#8211; Demonstrate conceptually or by prototype the values implications of particular design choices in particular systems.<br />
&#8211; Think rigorously, systematically, and critically about values in technical systems and devices.</p>
<p><strong>APPROACHES TO TEACHING AND LEARNING</strong></p>
<p>Classes will comprise a variety of activity including instructor presentation, group discussion of readings, and individual and small group presentation of mini-projects. Instructor and small group discussions will focus on concepts and arguments drawn from weekly reading assignments. You are encouraged to study together and to discuss information and concepts covered in lecture and the sections with other students.<br />
Cautions<br />
&#8211; Permissable cooperation does not include: One student copying all or part of work done by someone else. Or, one student signing the attandance sheet for another student.<br />
&#8211; Should copying occur, both the student who copied work from another student and the student who gave material to be copied will both automatically receive a zero for the assignment. Penalty for violation of this Code can also be extended to include failure of the course and University disciplinary action. In general, students in this course are expected to abide by the <a href="https://theuniversityfaculty.cornell.edu/academic-integrity/">Cornell University Code of Academic Integrity </a></p>
<p><strong>READINGS AND POSTING COMMENTS</strong></p>
<p>All required readings are posted on the Class Blackboard. Students should post comments on readings each Tuesday evening before class on Wednesday. Recommended readings (for students wishing to delve deeper into particular issues) are clearly marked.</p>
<table>
<tbody>
<tr>
<td><strong>SCHEDULE</strong></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Aug 29</strong></td>
<td><strong>Introduction to the Course</strong></td>
</tr>
<tr>
<td></td>
<td><em>Readings:</em> no assigned readings for first session.</td>
</tr>
<tr>
<td><strong>Sep 5</strong></td>
<td><strong>Ethical Thinking: Values in Design</strong></td>
</tr>
<tr>
<td></td>
<td>Langdon Winner&#8217;s claim that technologies have politics has inspired generations of scholars and designers &#8212; those who believe it to be true as well as those who seek to disprove it. Winner&#8217;s idea serve as a jumping off point for ethical thinking about values in design. Because of its centrality, we study the article, carefully and critically. As you read, ask what views the Winner article contradicts? We consider VID as a tool for analysis and as a guide to practice.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Winner, L. &#8220;Do Artifacts Have Politics?&#8221; <em>The Whale and the Reactor</em>. Chicago: The University of Chicago Press, 1986, 19-39</td>
</tr>
<tr>
<td></td>
<td>2. Postman, N. &#8220;Five Things We Need to Know About Technological Change.&#8221;</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Sep 12</strong></td>
<td><strong>Bias, Fairness, Discrimination &#8211; Part 1</strong></td>
</tr>
<tr>
<td></td>
<td>Bias and unfair discrimination are among the most serious worries over increasing reliance on computerized control and decision-making. In the first class sessions devoted to this topic (there are others), we will examine older instances of this concern, not applied to big data and AI, the most serious contemporary concern. Through these early cases, however, we will think through the nature of bias, why it is concerning from an ethical perspective, and what is distinctive about bias when it is embodied in technical systems.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Friedman, B. &amp; Nissenbaum, H. &#8220;Bias in Computer Systems.&#8221; <em>ACM Transactions on Information Systems</em> 14:3 (1996): 330-347</td>
</tr>
<tr>
<td></td>
<td>2. Weber, R. &#8220;Manufacturing Gender in Military Cockpit Design.&#8221; <em>The Social Shaping of Technology</em>. Eds. MacKenzie, D. and J. Wajcman. Milton Keynes: Open University Press, 1985.</td>
</tr>
<tr>
<td></td>
<td>3. Introna, L. and H. Nissenbaum, &#8220;Defining the Web: The Politics of Search Engines,&#8221; IEEE Computer, 33:1, 54-62</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Sep 19</strong></td>
<td><strong>Philosophical Foundations</strong></td>
</tr>
<tr>
<td></td>
<td>This week we take a look at some theoretical tools from philosophy that can supply a foundation for ethical reasoning. We&#8217;ll discuss how we can tell when we&#8217;re facing an ethical problem and on what grounds we might defend an ethical position. As an example case, we&#8217;ll consider the decision that Christopher Wylie made to become a whistleblower against Cambridge Analytica.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Fieser, James. (n.d.) <a href="https://www.iep.utm.edu/ethics/">&#8220;Ethics.&#8221; Internet Encyclopedia of Philosophy</a> (Only these excerpts: First two paragraphs; Section 2 Normative Ethics; Section 3a. Normative Principles in Applied Ethics).</td>
</tr>
<tr>
<td></td>
<td>2. Cadwalladr, C. (18 March, 2018). &#8220;&#8216;I made Steve Bannon&#8217;s psychological warfare tool&#8217;: meet the data war whistleblower.&#8221; <em>The Guardian</em>.</td>
</tr>
<tr>
<td></td>
<td>3. Also watch the <a href="https://www.theguardian.com/news/2018/mar/17/data-war-whistleblowerchristopher-wylie-faceook-nix-bannon-trump">video interview with Christopher Wylie</a></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Sep 26</strong></td>
<td><strong>Privacy &#8211; Part I</strong></td>
</tr>
<tr>
<td></td>
<td>It is no surprise that privacy has remained one of the central themes associated with digital technologies. These technologies have excelled at monitoring people, recording, storing, and organizing data, and enabling inference and analysis through computational capacities. From data stored in mainframes, to networked servers, online platforms, mobile devices, and networked objects (IoT), the vectors for so doing have grown in breadth and power, expanded into new domains, and enabled practices, of great value to private industry and government. At the same time, they have multiplied threats to privacy. In this class period, small groups will review and discuss a handful of cases that have attracted public scrutiny. To prepare, read the article under Readings and the Cases, focusing on the starred ones.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Hoofnagle, C.J., Soltani, A., Good, N., Wambach, D.J., and Ayenson, M.D. &#8220;Behavioral Advertising: The Offer You Cannot Refuse.&#8221; <em>Harvard Law &amp; Policy Review</em> 6, 273 (2012).</td>
</tr>
<tr>
<td></td>
<td><strong>Cases:</strong></td>
</tr>
<tr>
<td></td>
<td>*&#8221;An Intentional Mistake: the Anatomy of Google&#8217;s Wi-Fi Sniffing Debacle&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;How Google Collected Data from Wi-Fi Networks&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;Yahoo Scans Emails for Data to Sell&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;Thirty-One Privacy and Civil Liberties Organizations Urge Google to Suspend Gmail&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;The Fuss About Gmail and Privacy&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;How Game Apps that Captivate Kids Have been Collecting Their Data&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;What Walmart Knows About Customers&#8217; Habits&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;Facebook Delivers a Confident Sales Pitch to Advertisers&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;You for Sale&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;Banks and Retailers are Tracking How you Click, Swipe, and Tap&#8221;</td>
</tr>
<tr>
<td></td>
<td>*<a href="https://www.npr.org/sections/alltechconsidered/2014/02/25/282523377/thermal-imaging-gets-more-common-but-the-courts-havent-caught-up">&#8220;Thermal imaging gets more common but the courts haven&#8217;t caught up&#8221;</a></td>
</tr>
<tr>
<td></td>
<td>*&#8221;Accuweather caught sending user location data, even when sharing is off&#8221;</td>
</tr>
<tr>
<td></td>
<td>*&#8221;Why is this company tracking where you are on Thanksgiving?&#8221;</td>
</tr>
<tr>
<td></td>
<td>*<a href="https://apnews.com/828aefab64d4411bac257a07c1af0ecb/AP-Exclusive:-Google-tracks-your-movements,-like-it-or-not">&#8220;Google tracks your movements, like it or not&#8221;</a></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Oct 3</strong></td>
<td><strong>The Practical Turn: Values at Play I</strong></td>
</tr>
<tr>
<td></td>
<td>For some, the idea is obvious: if technical systems and devices embody values, the designers and creators of these technologies should be able to take a proactive stance and think about values in the process of developing technologies. At this pivotal point in the course, we consider how to take values into consideration and at the same time collectively think in more concrete terms about your projects. There are also theoretically inspired challenges to the practical turn, which we will address in later weeks. Privacy will serve as an application focus.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Flanagan, M. and H. Nissenbaum, <em>Values at Play in Digital Games</em>, Cambridge: MIT Press, 2014. (See Excerpts 1 and 2)</td>
</tr>
<tr>
<td></td>
<td>2. Berlin, I. &#8220;The Crooked Timber of Humanity.&#8221; (1991) <em>The Crooked Timber of Humanity: Chapters in the History of Ideas</em>. Ed. H. Hardy. New York: Knopf, 1-19.</td>
</tr>
<tr>
<td></td>
<td>3. Perry, J., Macken, E., Scott, N. and J. McKinley. &#8220;Disability, Inability, and Cyberspace.&#8221; <em>Human Values and the Design of Computer Technology</em>. Ed. Batya Friedman. New York: Cambridge University Press, 1997. 65-90.</td>
</tr>
<tr>
<td></td>
<td>4. Bentham, J. &#8220;Panopticon; or the Inspection House.&#8221; (1791) The Panopticon Writings. New York: Verso, 1995. 31-37.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Oct 10</strong></td>
<td><strong>Bias, Fairness, Discrimination &#8211; Part II</strong></td>
</tr>
<tr>
<td></td>
<td>Bias in search engines was the tip of the iceberg. As big data yielded data science and machine learning yielded algorithmic decisions and AI, experts have warned that unfair discrimination is not merely an accidental outcome of poorly thought-out automated decision systems, particularly applied to people in societal and institutional settings, but is possibly an inevitable outcome of such systems. At every stage of development, including data collection, selection, measurement, analysis, and learning to situated implementation unfairness lurks, not least because we live in societies where historical and institutional unfairness is rife. Because this issue has inflamed an enormous body of research, in a very short time, we will manage to dip into only a few of the illustrative cases.</td>
</tr>
<tr>
<td></td>
<td>Guest lecturer: Professor Solon Barocas</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Barocas, S., M. Hardt, A. Narayanan (Draft) Fairness and Machine Learning: Limitations and Opportunities, <a href="https://fairmlbook.org/introduction.html">Chapter One (Introduction) </a></td>
</tr>
<tr>
<td></td>
<td>Cases: (Recommended, for now)</td>
</tr>
<tr>
<td></td>
<td>Valentino-Devries, J., Singer-Vine, J., and Soltani, A. &#8220;Websites Vary Prices, Deals Based on Users&#8217; Information.&#8221; <em>Wall Street Journal</em>, 24 Dec 2012.</td>
</tr>
<tr>
<td></td>
<td>&#8220;Racism is Poisoning Online Ad Delivery, says Harvard Professor.&#8221; <em>MIT Technology Review</em>, February 4, 2013.</td>
</tr>
<tr>
<td></td>
<td>Miller, C. &#8220;Can an Algorithm Hire Better than a Human?&#8221; <em>The New York Times</em>. June 28, 2015.</td>
</tr>
<tr>
<td></td>
<td>Mann, G. and O&#8217;Neil, C. <a href="https://hbr.org/2016/12/hiring-algorithms-are-not-neutral">&#8220;Hiring Algorithms are Not Neutral.&#8221;</a> Harvard Business Review. December 9, 2016.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Oct 17</strong></td>
<td><strong>Accountability, Transparency, Explanation</strong></td>
</tr>
<tr>
<td></td>
<td>At a minimum, ethics requires that we take responsibility for our actions. In just and decent societies, people who are subjected to the decisions and actions of others are owed explanations and those who hold power over the lives and interests others should be held to account and not be allowed to exercise these powers arbitrarily. As institutional action is increasingly automated via computer and algorithmic systems, these foundational assumptions are increasingly challenged &#8212; as functionality is spread across multiple unfamiliar actors, as algorithmic systems are difficult, if not impossible to comprehend, and as economic and political incentives to automate seem to outweigh ages-old commitments to accountability, transparency, and protections again tyranny.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Nissenbaum, H. &#8220;Accountability in a Computerized Society,&#8221; in <em>Science and Engineering Ethics</em>, 1996, 2, 25-42</td>
</tr>
<tr>
<td></td>
<td>2. Citron, D. and F. Pasquale, &#8220;The Scored Society: Due Process for Automated Precisions?&#8221; Univ. of Maryland Francis King Carey School of Law, Legal Studies Research Paper, No. 2014-8</td>
</tr>
<tr>
<td></td>
<td>3. Bornstein, A. &#8220;Is Artificial Intelligence Permanently Inscrutable? Despite new biology-like tools, some insist interpretation is impossible.&#8221; (2016). <em>Nautilus</em> No. 40.</td>
</tr>
<tr>
<td></td>
<td>4. Wakabayashi, D. &#8220;Self-Driving Uber Car Kills Pedestrian in Arizona, Where Pedestrians Roam.&#8221; <em>The New York Times</em>. March 19, 2018.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Oct 24</strong></td>
<td><strong>Privacy &#8211; Part II</strong></td>
</tr>
<tr>
<td></td>
<td>This session focuses on a family of privacy challenges arising data science and big data. In addition, it includes an overview of the landscape of privacy theory and regulation, with a focus on the theory of privacy as contextual integrity.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Duhigg, C. &#8220;How Companies Learn Your Secrets.&#8221; <em>The New York Times</em>. Feb 16, 2012.</td>
</tr>
<tr>
<td></td>
<td>2. Pangborn, P.J. &#8220;Even This Data Guru is Creeped Out by What Anonymous Location Data Reveals About Us,&#8221; <em>Fast Company</em>, Sept. 9, 2017</td>
</tr>
<tr>
<td></td>
<td>3. Al_Jazeera &#8220;Terms of Service: Understanding our role in the world of Big Data&#8221;</td>
</tr>
<tr>
<td></td>
<td>4. Nissenbaum, H. (2015) <a href="https://nissenbaum.tech.cornell.edu/papers/Respect%20for%20Context%20Fulfilling%20the%20Promise%20of%20the%20White%20House%20Report.pdf">&#8220;&#8216;Respect for Context&#8217;: Fulfilling the Promise of the White House Report,&#8221;</a> In <em>Privacy in the Modern Age: The Search for Solutions</em>, Eds. M. Rotenberg, J. Horwitz, J. Scott, EPIC, New York: EPIC/The New Press, 152-164.</td>
</tr>
<tr>
<td></td>
<td>5. Singer, N. <a href="http://www.nytimes.com/2014/06/29/technology/when-a-health-plan-knows-how-you-shop.html">&#8220;When a Health Plan Knows How You Shop,&#8221;</a> <em>The New York Times</em>, July 6, 2014. .</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Oct 31</strong></td>
<td><strong>The Practical Turn: Values at Play II</strong></td>
</tr>
<tr>
<td></td>
<td>We continue thinking about the role of &#8220;conscientious&#8221; designers and developers in creating technical systems that embody and promote ethical and political values. We will consider some of the theoretical assumption behind this approach and will work through applications of it to particular cases.</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Flanagan, M. and H. Nissenbaum, <em>Values at Play in Digital Games</em>, Cambridge: MIT Press, 2014. (See Excerpts 1 and 2)</td>
</tr>
<tr>
<td></td>
<td>2. Weinberg, A. M. &#8220;Can Technology Replace Social Engineering.&#8221; <em>Controlling Technology: Contemporary Issues</em>. Ed. W. B. Thompson. Buffalo, NY: Prometheus Books, 1991. 41-48.</td>
</tr>
<tr>
<td></td>
<td>3. Berlin, I. &#8220;The Crooked Timber of Humanity.&#8221; (1991) <em>The Crooked Timber of Humanity: Chapters in the History of Ideas</em>. Ed. H. Hardy. New York: Knopf, 1-19.</td>
</tr>
<tr>
<td></td>
<td>4. Perry, J., Macken, E., Scott, N. and J. McKinley. &#8220;Disability, Inability, and Cyberspace.&#8221; <em>Human Values and the Design of Computer Technology</em>. Ed. Batya Friedman. New York: Cambridge University Press, 1997. 65-90.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Nov 7</strong></td>
<td><strong>Digital Manipulation</strong></td>
</tr>
<tr>
<td></td>
<td>We usually think of humans (scientists and engineers) as shapers of technology. But, in turn, technology, architecture, mechanism, and design may shape humans &#8212; affording and constraining certain behaviors, changing our beliefs, provoking critical reflection, encouraging better habits, and making us better people. All well and good, but what about technology that may afford and constrain in questionable ways, that may offer the means of exploitation and manipulation? Humans manipulating one another is probably as old as social life itself; how is it different when enacted or mediated by software?</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Harris T. &#8220;How Technology Hijacks People&#8217;s Minds &#8211; from a Magician and Google Design Ethicist.&#8221; (2016). <em>Medium.com</em>.</td>
</tr>
<tr>
<td></td>
<td>2. Scheiber, N. <a href="https://www.nytimes.com/interactive/2017/04/02/technology/uber-drivers-psychological-tricks.html">&#8220;How Uber Uses Psychological Tricks to Push Its Drivers&#8217; Buttons,&#8221;</a> <em>The New York Times</em>, April 2, 2017.</td>
</tr>
<tr>
<td></td>
<td>3. Scheiber, N. <a href="https://www.nytimes.com/2016/05/15/technology/personaltech/when-websites-wont-take-no-for-an-answer.html">&#8220;When Websites Won&#8217;t Take No For an Answer&#8221;</a> <em>The New York Times</em>, May 14, 2016.</td>
</tr>
<tr>
<td></td>
<td>4. Levin, T. &#8220;Facebook told advertisers it can identify teens feeling &#8216;insecure&#8217; and &#8216;worthless&#8217;.&#8221; <em>The Guardian</em>. May 1, 2017</td>
</tr>
<tr>
<td></td>
<td>5. Paskova, Y. &#8220;OKCupid Plays with Love in User Experiments.&#8221; <em>The New York Times</em>, July 28, 2017.</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Nov 14</strong></td>
<td><strong>Resisting Technology with Technology</strong></td>
</tr>
<tr>
<td></td>
<td>When is it justifiable to take matters into one&#8217;s own hands to express protest and resist systems with which one disagrees?</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Gurses, S, R. Overdorf, E. Balsa (2018) &#8220;Stirring the POTs: Protective Optimization Technologies&#8221; [DRAFT short version]</td>
</tr>
<tr>
<td></td>
<td>2. OR, Overdorf, R., B. Kylunych, E. Balsa, C. Troncoso, S. Gurses &#8220;POTs: Protective Optimization Technologies,&#8221; [arXiv: 1806.02711v.3 [cs.CY] 8/30/2018 [longer, more technical]</td>
</tr>
<tr>
<td></td>
<td>3. Kantor, J. and S. Hodgson, <a href="https://www.nytimes.com/interactive/2014/08/13/us/starbucks-workers-scheduling-hours.html?_r=0">&#8220;Working anything but 9 to 5,&#8221;</a> <em>The New York Times</em>, August 13, 2014.</td>
</tr>
<tr>
<td></td>
<td>4. Marx, G. &#8220;A Tack in the Shoe: Neutralizing and Resisting the New Surveillance,&#8221; <em>Journal of Social Issues</em>, Vol. 59, No.2, 2003, 369-390.</td>
</tr>
<tr>
<td></td>
<td>5. <a href="https://adnauseam.io/">https://adnauseam.io/</a></td>
</tr>
<tr>
<td></td>
<td>6. <a href="https://cs.nyu.edu/trackmenot/">https://cs.nyu.edu/trackmenot/</a></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Nov 21</strong></td>
<td><strong>Thanksgiving Recess</strong></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Nov 28</strong></td>
<td><strong>From Content Moderation to Censorship</strong></td>
</tr>
<tr>
<td></td>
<td>Many social media companies, including Twitter, Youtube, and Facebook, provide services that require them to make decisions about what content to permit and to prioritize on their platforms. They may ban users for abusive posts, delete messages that promotes terrorism, place restrictions on pornography, recommend relevant content, etc. Some of those decisions are made directly by humans; others are made by algorithms. Regardless, such decisions often involve difficult value tradeoffs, diverse stakeholders, and conflicting interests. To design policies that guide such decisions, companies must consider values like freedom of speech, freedom of association, well-being, social welfare, democracy, public safety and security, as well as other factors, like company goals, legal obligations, obligations to shareholders, reputation, and financial interests. In the face of all these considerations, what content moderation policies should social media companies adopt?</td>
</tr>
<tr>
<td></td>
<td><em>Readings</em>:</td>
</tr>
<tr>
<td></td>
<td>1. Marantz, A. <a href="https://www.newyorker.com/magazine/2018/03/19/reddit-and-the-struggle-to-detoxify-the-internet">&#8220;Reddit and the Struggle to Detoxify the Internet,&#8221;</a> <em>The New Yorker</em>, March 19, 2018.</td>
</tr>
<tr>
<td></td>
<td>2. Taub, A. and Fisher, M., <a href="https://www.nytimes.com/2018/04/21/world/asia/facebook-sri-lanka-riots.html">&#8220;Where Countries Are Tinderboxes and Facebook Is a Match,&#8221;</a> <em>The New York Times</em>, April 24, 2018.</td>
</tr>
<tr>
<td></td>
<td>3. Diresta, R., <a href="https://www.wired.com/story/creating-ethical-recommendation-engines/">&#8220;Up Next: A better recommendation system,&#8221;</a> <em>Wired</em>, April 11, 2018.</td>
</tr>
<tr>
<td></td>
<td>4. (Read/browse, but don&#8217;t summarize for the reading reflection): Jack, C. <a href="https://datasociety.net/pubs/oh/DataAndSociety_LexiconofLies.pdf">&#8220;Lexicon of Lies: Terms for Problematic Information,&#8221;</a> <em>Data &amp; Society</em>, August 9, 2017.</td>
</tr>
</tbody>
</table>
]]></content:encoded>
							<wfw:commentRss>https://valuesindesign.wordpress.com/2019/08/19/ethical-thinking-about-digital-technologies-and-data/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:thumbnail url="https://valuesindesign.files.wordpress.com/2014/07/p1150641.jpg" />
		<media:content url="https://valuesindesign.files.wordpress.com/2014/07/p1150641.jpg" medium="image">
			<media:title type="html">P1150641</media:title>
		</media:content>

		<media:content url="https://2.gravatar.com/avatar/84b26454cc393776e28d4af11aadc1e4?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">cqx931</media:title>
		</media:content>
	</item>
		<item>
		<title>ETHICS AND POLICY IN DATA SCIENCE</title>
		<link>https://valuesindesign.wordpress.com/2019/08/12/ethics-and-policy-in-data-science/</link>
				<comments>https://valuesindesign.wordpress.com/2019/08/12/ethics-and-policy-in-data-science/#respond</comments>
				<pubDate>Mon, 12 Aug 2019 18:09:10 +0000</pubDate>
		<dc:creator><![CDATA[Sally Chen]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://valuesindesign.wordpress.com/?p=508</guid>
				<description><![CDATA[ Solon Barocas (Professor) Brian McInnis (Teaching Assistant) &#160; COURSE DESCRIPTION AND OBJECTIVES &#160; This class will teach you to recognize...]]></description>
								<content:encoded><![CDATA[<p><strong> </strong><strong>Solon Barocas (Professor)</strong></p>
<p><strong>Brian McInnis (Teaching Assistant)</strong></p>
<p>&nbsp;</p>
<p><strong>COURSE DESCRIPTION AND OBJECTIVES</strong></p>
<p>&nbsp;</p>
<p>This class will teach you to recognize where and understand why ethical issues and policy questions can arise when applying data science to real world problems. It will bring analytic and technical precision to normative debates about the role that data science, machine learning, and artificial intelligence play in consequential decision-making in commerce, employment, finance, healthcare, education, policing, and other areas. We will focus on ways to conceptualize, measure, and mitigate bias in data-driven decision-making, to audit and evaluate models, and to render these analytic tools more interpretable and their determinations more explainable. You will learn to think critically about how to plan, execute, and evaluate a project with these concerns in mind, and how to cope with novel challenges for which there are often no easy answers or established solutions.</p>
<p>&nbsp;</p>
<p>To do so, you will develop fluency in the key technical, ethical, policy, and legal terms and concepts that are relevant to a normative assessment of data science; learn about some of the common approaches and emerging tools for mitigating or managing these ethical concerns; and gain exposure to legal scholarship and policy documents that will help you understand the current regulatory environment and anticipate future developments. Ultimately, the class will teach you how to reason through these problems in a systematic manner and how to justify and defend your approach to dealing with them.</p>
<p>&nbsp;</p>
<p><strong>COURSE MATERIALS</strong></p>
<p>&nbsp;</p>
<p>All course materials will be available on Blackboard.</p>
<p>&nbsp;</p>
<p>We will read critical commentary and thoughtful reflections by seasoned practitioners, important and illustrative research from computer scientists, an interesting mix of legal scholarship, moral philosophy, and policy analysis, and a host of government documents. All along the way, we will rely on case studies, recent controversies, and current events to ground our discussion.</p>
<p>&nbsp;</p>
<p>The appropriate response to many of the problems that we will address in the course is far from settled. This is, consequently, a reading-heavy course. Even so, the assigned readings frequently do not present all sides of the debate. I have therefore selected materials that tend to offer a more critical—and sometimes less familiar—perspective with the goal of provoking productive debate during our class and strong reactions in your assignments. I expect you to stake out conflicting—informed and carefully reasoned—positions on the issues, and you should not shy away from doing so.</p>
<p>&nbsp;</p>
<p>The lecture, discussion, and in-class activities will cover most, but not all of the issues raised by the readings. Given the nature of the issues and material under consideration, I expect lively debate and plan to follow the natural flow of discussion as much as possible. As such, I am certain that class will cover some important ideas that do not appear in the readings. Active listening and participation is therefore crucial.</p>
<p>&nbsp;</p>
<p><strong>ASSIGNMENTS</strong></p>
<p>&nbsp;</p>
<ul>
<li>Answer a brief question at the start of class — OngoingWhen you arrive in class, you will find a question on the chalkboard. You will answer the question on Piazza. While I encourage you to take a moment to carefully consider the question and give a thoughtful answer, your response can be brief. The goal is to jumpstart your thinking. You must, however, submit your response before you leave class, as your submission will be a way to document your attendance.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>Post to the Blackboard discussion board — OngoingAs a matter of course, you should post any interesting news items that you happen to read to the Blackboard discussion board—and take a moment to reflect and comment on their significance and relevance to our class. Items directly related to the reading assigned at that time are especially welcome. While this is voluntary, I will make sure that your contributions to the discussion board are reflected in your participation grade.</li>
<li>Critically assess a proposed data science project — Due September 29Drawing on the readings from the first third of the class, you will critically assess a proposed data science project. I will provide you with a brief description of the project and you will identify 3 potential problems with the proposed application that could raise concerns with fairness. Your answer should take the form of 3 bullet points, each comprising 2-4 sentences: 1-2 sentences identifying the source of the problem, and 1-2 sentences explaining how fairness is at stake. The problems that you identify should be as distinct as possible to receive maximum credit.</li>
<li>Respond to the Consumer Financial Protection Bureau’s <a href="http://files.consumerfinance.gov/f/documents/20170214_cfpb_Alt-Data-RFI.pdf">Request for Information Regarding Use of Alternative Data and Modeling Techniques in the Credit Process</a> — Due November 10The Consumer Financial Protection Bureau is currently considering a number of policy questions raised by novel forms of credit scoring that rely on new sources of data and more sophisticated learning methods. The Bureau has solicited input from outside experts and the broader public, with the aim of better understanding how to deal with issues ranging from privacy to non-discrimination and the ability to explain credit decisions. Drawing on the course readings and ideas discussed in class, you will draft a 3-5 page, double-spaced response to the Bureau’s request, staking out and advocating in favor of a particular policy position. You should further support your position by explaining the strategies and tools currently available to address the Bureau’s concerns.</li>
<li>Write a final paper that revisits a recent controversy — Due December 12In a 8-10 page, double-spaced paper, you will revisit a recent controversy involving different course themes. You will choose from a set of <a href="https://docs.google.com/document/d/1jooNmZxBpe8CxQx7Fhk2hbASFZnbMk5HTsZqz9E0Kwg/edit?usp=sharing">5 predetermined cases</a>. Your paper should draw extensively from the course materials, lectures, and in-class discussions, and present a comprehensive plan for undertaking the project in a way that addresses fairness, respects privacy (as a legal and broader normative matter), and comports with other pertinent ethical principles. The paper should not shy away from pointing out difficult tensions or unavoidable trade-offs. You should instead explain why it is not possible to “have it all,” and then provide a thoughtful justification for the specific trade-off that you suggest.</li>
</ul>
<p>&nbsp;</p>
<p><strong>SUBMITTING ASSIGNMENTS</strong></p>
<p>&nbsp;</p>
<ul>
<li>All assignments must be submitted through Blackboard. Do not email or physically hand in any assignments. Always confirm that your assignment has uploaded correctly after submission.</li>
<li>Should you encounter a problem with Blackboard, please email the TA before the deadline with (1) your completed assignment, (2) a screenshot of the problem, and (3) the time of your attempted submission.</li>
<li>You will incur a 20% penalty if you submit your work within 24 hours after the deadline. You will receive no credit thereafter.</li>
<li>There are no exceptions to this late submission policy, except university-approved excuses.</li>
<li>Upon receiving your graded assignment, please take at least 24 hours to consider the feedback you have received as well as the original assignment instructions. If, at that time, you feel that you deserved a better grade, you may submit a formal written request to the TA by email. Your request must explain exactly where you believe there was a mistake in grading or why you object to a specific piece of feedback. The TA will consider well justified requests and re-grade assignments as appropriate.</li>
</ul>
<p>&nbsp;</p>
<p><strong>GRADING</strong></p>
<p>&nbsp;</p>
<p>20% Participation (both in-class and on Blackboard)</p>
<p>15% Critical review of proposed data science project</p>
<p>25% Response to Consumer Financial Protection Bureau’s Request for Information</p>
<p>40% Final paper revisiting a recent controversy</p>
<p>&nbsp;</p>
<p><strong>ACADEMIC INTEGRITY</strong><br />
I expect you to abide by Cornell’s Code of Academic Integrity at all times. Please note that the Code specifically states that a “Cornell student&#8217;s submission of work for academic credit indicates that the work is the student&#8217;s own. All outside assistance should be acknowledged, and the student&#8217;s academic position truthfully reported at all times.”</p>
<p>&nbsp;</p>
<p>Please contact me or the TA if you have any questions or concerns about appropriately acknowledging others’ work in your submitted assignments. You should expect that I will rigorously enforce the Code and may use software to check for plagiarism.</p>
<p>&nbsp;</p>
<p><strong>SCHEDULE AND READINGS</strong></p>
<p>&nbsp;</p>
<p>I expect you to complete all assigned readings prior to class. Unless I’ve noted particular parts, sections, or pages for you to read, you should read the assigned text in its entirety. For some classes, I have listed <em>recommended</em> readings that you may choose to complete, if you are so inclined. These are optional, and I will not expect that you have read them.</p>
<p>&nbsp;</p>
<p>The schedule and readings are subject to change as we progress through the semester. Please always refer to the syllabus posted to Blackboard before you begin reading for the next class.</p>
<p>&nbsp;</p>
<p>Background Reading [Optional]</p>
<p>&nbsp;</p>
<ul>
<li>Boyd and Crawford, “Critical Questions for Big Data”</li>
<li>Zarsky, “The Trouble with Algorithmic Decisions”</li>
<li>O&#8217;Neil, <em>Weapons of Math Destruction</em></li>
<li>Pasquale, <em>The Black Box Society</em></li>
<li>The White House Office of Science and Technology Policy, <em>Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights</em></li>
</ul>
<p>&nbsp;</p>
<p><strong>August 23</strong> — Welcome</p>
<p>&nbsp;</p>
<p><strong>August 28</strong> — <em>Data</em>, the givens</p>
<p>&nbsp;</p>
<ul>
<li>Gitelman and Jackson, <em>Raw Data is an Oxymoron</em> [Introduction]</li>
<li>Agre, “Surveillance and Capture: Two Models of Privacy”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Bowker and Star, <em>Sorting Things Out</em></li>
<li>Auerbach, “The Stupidity of Computers”</li>
</ul>
<p>&nbsp;</p>
<p><strong>August 30</strong> — What problem are we solving?</p>
<p>&nbsp;</p>
<ul>
<li>Moor, “What is Computer Ethics?”</li>
<li>Hand, “Deconstructing Statistical Questions”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 4</strong> — Labor Day — No class</p>
<p>&nbsp;</p>
<p><strong>September 6</strong> — Cultivating a critical disposition</p>
<p>&nbsp;</p>
<ul>
<li>O’Neil, <em>On Being a Data Skeptic</em></li>
<li>Domingos, “A Few Useful Things to Know About Machine Learning”Recommended</li>
<li>Luca, Kleinberg, and Mullainathan, “Algorithms Need Managers, Too”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 11</strong> — Bias and exclusion</p>
<p>&nbsp;</p>
<ul>
<li>Friedman and Nissenbaum, “Bias in Computer Systems”</li>
<li>Lerman, “Big Data and Its Exclusions”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Hand, “Classifier Technology and the Illusion of Progress” [Sections 3 and 4]</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 13</strong> — The social science of discrimination</p>
<p>&nbsp;</p>
<ul>
<li>Pager and Shepherd, “The Sociology of Discrimination: Racial Discrimination in Employment, Housing, Credit, and Consumer Markets”</li>
<li>Goodman, “Economic Models of (Algorithmic) Discrimination”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 18</strong> — How machines learn to discriminate</p>
<p>&nbsp;</p>
<ul>
<li>Hardt, “How Big Data Is Unfair”</li>
<li>Barocas and Selbst, “Big Data’s Disparate Impact” [Parts I and II]</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Gandy, “It’s Discrimination, Stupid”</li>
<li>Dwork and Mulligan, “It’s Not Privacy, and It&#8217;s Not Fair”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 20</strong> — Auditing algorithms</p>
<p>&nbsp;</p>
<ul>
<li>Sandvig, Hamilton, Karahalios, and Langbort, “Auditing Algorithms: Research Methods for Detecting Discrimination on Internet Platforms”</li>
</ul>
<ul>
<li>Diakopoulos, “Algorithmic Accountability: Journalistic Investigation of Computational Power Structures”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Lavergne and Mullainathan, “Are Emily and Greg more Employable than Lakisha and Jamal?”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 25</strong> — Algorithms audited</p>
<p>&nbsp;</p>
<ul>
<li>Sweeney, “Discrimination in Online Ad Delivery”</li>
</ul>
<ul>
<li>Datta, Tschantz, and Datta, “Automated Experiments on Ad Privacy Settings”</li>
</ul>
<p>&nbsp;</p>
<p><strong>September 27</strong> — Formalizing and enforcing fairness</p>
<p>&nbsp;</p>
<ul>
<li>Dwork, Hardt, Pitassi, Reingold, and Zemel, “Fairness Through Awareness”</li>
<li>Feldman, Friedler, Moeller, Scheidegger, and Venkatasubramanian, “Certifying and Removing Disparate Impact”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Žliobaitė and Custers, “Using Sensitive Personal Data May Be Necessary for Avoiding Discrimination in Data-Driven Decision Models”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 2</strong> — Accounting for disparities in accuracy and error rates [Manish Raghavan, a doctoral student in computer science at Cornell and co-author of one of the assigned readings, will join us for this class]</p>
<p>&nbsp;</p>
<ul>
<li>Angwin, Larson, Mattu, and Kirchner, “Machine Bias”</li>
<li>Kleinberg, Mullainathan, and Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Northpointe, <em>COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity</em></li>
<li>Chouldechova, “Fair Prediction with Disparate Impact”</li>
<li>Berk, Heidari, Jabbari, Kearns, and Roth, “Fairness in Criminal Justice Risk Assessments: The State of the Art”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 4</strong> — Competing notions of fairness</p>
<p>&nbsp;</p>
<ul>
<li>Hardt, Price, and Srebro, “Equality of Opportunity in Supervised Learning”
<ul>
<li>Wattenberg, Viégas, and Hardt, “Attacking Discrimination with Smarter Machine Learning”</li>
</ul>
</li>
<li>Friedler, Scheidegger, and Venkatasubramanian, “On the (Im)possibility of Fairness”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Tene and Polonetsky, “Taming the Golem: Challenges of Ethical Algorithmic Decision Making”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 9</strong> — Fall break — No class</p>
<p>&nbsp;</p>
<p><strong>October 11</strong> — Feedback loops and fairness</p>
<p>&nbsp;</p>
<ul>
<li>Lum and Isaac, “To Predict and Serve?”</li>
<li>Joseph, Kearns, Morgenstern, and Roth, “Fairness in Learning: Classic and Contextual Bandits”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 16</strong> — The fairness of different factors</p>
<p>&nbsp;</p>
<ul>
<li>Barocas, “Data Mining and the Discourse on Discrimination”</li>
<li>Grgić-Hlača, Zafar, Gummadi, and Weller, “The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 18</strong> — Profiling and particularity</p>
<p>&nbsp;</p>
<ul>
<li>Vedder, “KDD: The Challenge to Individualism”</li>
<li>Lippert-Rasmussen, “‘We Are All Different’: Statistical Discrimination and the Right to Be Treated as an Individual”</li>
</ul>
<p><em>Recommended</em></p>
<ul>
<li>Schauer, <em>Profiles, Probabilities, And Stereotypes</em></li>
</ul>
<p>&nbsp;</p>
<p><strong>October 23</strong> — From allocative to representational harms</p>
<p>&nbsp;</p>
<ul>
<li>Caliskan, Bryson, and Narayanan, “Semantics Derived Automatically from Language Corpora Contain Human-like Biases”</li>
<li>Zhao, Wang, Yatskar, Ordonez, and Chang, “Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Bolukbasi, Chang, Zou, Saligrama, and Kalai, “Man Is to Computer Programmer as Woman Is to Homemaker?”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 25</strong> — Transparency and due process</p>
<p>&nbsp;</p>
<ul>
<li>Citron and Pasquale, “The Scored Society: Due Process for Automated Predictions”</li>
<li>Ananny and Crawford, “Seeing without Knowing”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>de Vries, “Privacy, Due Process and the Computational Turn”</li>
<li>Zarsky, “Transparent Predictions”</li>
<li>Crawford and Schultz, “Big Data and Due Process”</li>
<li>Kroll, Huey, Barocas, Felten, Reidenberg, Robinson, and Yu, “Accountable Algorithms”</li>
</ul>
<p>&nbsp;</p>
<p><strong>October 30</strong> — Interpretability in machine learning</p>
<p>&nbsp;</p>
<ul>
<li>Bornstein, “Is Artificial Intelligence Permanently Inscrutable?”</li>
<li>Burrell, “How the Machine &#8216;Thinks&#8217;”</li>
<li>Lipton, “The Mythos of Model Interpretability”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Doshi-Velez and Kim, “Towards a Rigorous Science of Interpretable Machine Learning”</li>
<li>Hall, Phan, and Ambati, “Ideas on Interpreting Machine Learning”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 1</strong> — The value of explanation</p>
<p>&nbsp;</p>
<ul>
<li>Grimmelmann and Westreich, “Incomprehensible Discrimination”</li>
<li>Selbst and Barocas, “Regulating Inscrutable Systems”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Jones, “The Right to a Human in the Loop”</li>
<li>Edwards and Veale, “Slave to the Algorithm? Why a ‘Right to Explanation’ is Probably Not the Remedy You are Looking for”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 6</strong>  — The future of scoring</p>
<p>&nbsp;</p>
<ul>
<li>Robinson and Yu, <em>Knowing the Score</em></li>
<li>Hurley and Adebayo, “Credit Scoring in the Era of Big Data”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 8</strong> — The privacy implications of inference</p>
<p>&nbsp;</p>
<ul>
<li>Duhigg, “How Companies Learn Your Secrets”</li>
<li>Kosinski, Stillwell, and Graepel, “Private Traits and Attributes Are Predictable from Digital Records of Human Behavior”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Barocas and Nissenbaum, “Big Data&#8217;s End Run around Procedural Privacy Protections”</li>
<li>Chen, Fraiberger, Moakler, and Provost, “Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 13</strong> — Price discrimination</p>
<p>&nbsp;</p>
<ul>
<li>Valentino-Devries, Singer-Vine, and Soltani, “Websites Vary Prices, Deals Based on Users&#8217; Information”</li>
<li>The Council of Economic Advisers, <em>Big Data and Differential Pricing</em></li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Hannak, Soeller, Lazer, Mislove, and Wilson, “Measuring Price Discrimination and Steering on E-commerce Web Sites”</li>
<li>Kochelek, “Data Mining and Antitrust”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 15</strong> — Insurance</p>
<p>&nbsp;</p>
<ul>
<li>Helveston, “Consumer Protection in the Age of Big Data”</li>
<li>Kolata, “New Gene Tests Pose a Threat to Insurers”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Swedloff, “Risk Classification&#8217;s Big Data (R)evolution”</li>
<li>Cooper, “Separation, Pooling, and Big Data”</li>
<li>Simon, “The Ideological Effects of Actuarial Practices”</li>
</ul>
<p><strong>November 20</strong> — Algorithmic persuasion and manipulation</p>
<p>&nbsp;</p>
<ul>
<li>Tufekci, “Engineering the Public”</li>
<li>Calo, “Digital Market Manipulation”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Kaptein and Eckles, “Selecting Effective Means to Any End”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 22</strong> — Thanksgiving — No class</p>
<p>&nbsp;</p>
<p><strong>November 27</strong> — Algorithmic publics</p>
<ul>
<li>Pariser, “Beware Online ‘Filter Bubbles’”</li>
<li>Gillespie, “The Relevance of Algorithms”</li>
</ul>
<p>&nbsp;</p>
<p><strong>November 29</strong> — Rejecting certain applications of machine learning</p>
<p>&nbsp;</p>
<ul>
<li>Buolamwini, “Algorithms Aren’t Racist. Your Skin Is just too Dark”</li>
<li>Hassein, “Against Black Inclusion in Facial Recognition”</li>
<li>Agüera y Arcas, Mitchell, and Todorov, “Physiognomy’s New Clothes”</li>
</ul>
<p>&nbsp;</p>
<p><em>Recommended</em></p>
<ul>
<li>Garvie, Bedoya, and Frankle, <em>The Perpetual Line-Up</em></li>
<li>Wu and Zhang, “Automated Inference on Criminality using Face Images”</li>
<li>Haggerty, “Methodology as a Knife Fight”</li>
</ul>
<p><strong> </strong></p>
]]></content:encoded>
							<wfw:commentRss>https://valuesindesign.wordpress.com/2019/08/12/ethics-and-policy-in-data-science/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:thumbnail url="https://valuesindesign.files.wordpress.com/2014/07/about-2.png" />
		<media:content url="https://valuesindesign.files.wordpress.com/2014/07/about-2.png" medium="image">
			<media:title type="html">about-2</media:title>
		</media:content>

		<media:content url="https://2.gravatar.com/avatar/84b26454cc393776e28d4af11aadc1e4?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">cqx931</media:title>
		</media:content>
	</item>
		<item>
		<title>Values in Design Council</title>
		<link>https://valuesindesign.wordpress.com/2019/07/30/values-in-design-council/</link>
				<comments>https://valuesindesign.wordpress.com/2019/07/30/values-in-design-council/#respond</comments>
				<pubDate>Tue, 30 Jul 2019 16:08:40 +0000</pubDate>
		<dc:creator><![CDATA[emilygoldsher]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[NSF]]></category>
		<category><![CDATA[NSF future internet architecture project]]></category>
		<category><![CDATA[VID council]]></category>

		<guid isPermaLink="false">http://valuesindesign.wordpress.com/?p=322</guid>
				<description><![CDATA[Functioning as part of the NSF Future Internet Architecture Project, the Values in Design Council serves as a multi-disciplinary team...]]></description>
								<content:encoded><![CDATA[<p>Functioning as part of the <a href="http://www.nets-fia.net/">NSF Future Internet Architecture Project</a>, the <a title="Values in Design Council" href="https://valuesindesign.wordpress.com/initiatives-2/values-in-design-council/">Values in Design Council</a> serves as a multi-disciplinary team of experts in the social analysis of digital information technologies. The team acts as analysts and consultants to the FIA projects, helping to identify junctures in the design process in which values-critical technical decisions arise.</p>
<p>The Values In Design Council for Future Internet Architectures – Next Phase (VID4FIA-NP) will work alongside the project winners of the National Science Foundation’s Future Internet Architecture large awards. The aim is to build a unique forum for interposing questions and ideas about social, political, and ethical values in an ongoing dialog with computer scientists and engineers engaged in designing and developing Internet technologies.</p>
<p>The ideal VID4FIA-NP Council includes 16 members from different fields and career stages, whose work, one way or another, has been inspired by the notion that values, such as security, openness, accessibility, privacy, etc. are embodied in socio­technical systems, and further, that these values be considered not only post­ hoc with design patches and stopgap policy but fruitfully integrated throughout design and development. Next Phase awards, unlike the first phase of the projects, will be focused on applications, oriented around the specifics of the context of deployment.</p>
<p><strong>Council Members</strong></p>
<p><a href="http://www.ip.ethz.ch/people/bechtold">Stefan Bechtold</a>, Department of Humanities, Social and Political Sciences, ETH Zurich<br />
<a href="http://www.ics.uci.edu/~gbowker/">Geoffrey Bowker</a>, Donald Bren School of Information and Computer Sciences, UC Irvine<br />
<a href="http://finnb.net/">Finn Brunton</a> (PI), Department of Media, Culture, and Communication, NYU<br />
<a href="http://www.dourish.com/">Paul Dourish</a>, Donald Bren School of Information and Computer Sciences, UC Irvine<br />
<a href="http://www.tarletongillespie.org/">Tarleton Gillespie</a>, Department of Communication, Cornell University<br />
<a href="http://james.grimmelmann.net/">James Grimmelmann</a>, Francis King Carey School of Law, University of Maryland<br />
<a href="http://vous-etes-ici.net/">Seda Gurses</a>, Department of Media, Culture, and Communication, NYU<br />
<a href="https://www.law.berkeley.edu/php-programs/faculty/facultyProfile.php?facID=6494">Chris Hoofnagle</a>, Center for Law &amp; Technology, UC Berkeley<br />
<a href="http://rednoise.org/~dhowe/">Daniel C. Howe</a>, School of Creative Media, City University of Hong Kong<br />
<a href="http://www.amatwyshyn.com/">Andrea Matwyshyn</a>, Wharton School of Business, University of Pennsylvania<br />
<a href="http://www.ischool.berkeley.edu/people/faculty/deirdremulligan">Deirdre Mulligan</a>, School of Information, UC Berkeley<br />
<a href="http://www.nyu.edu/projects/nissenbaum/">Helen Nissenbaum</a> (PI), Department of Media, Culture, and Communication, NYU<br />
<a href="http://paulohm.com/">Paul Ohm</a>, University of Colorado Law School, University of Colorado at Boulder<br />
<a href="http://criticalmaking.com/matt-ratto/">Matt Ratto</a>, Faculty of Information, University of Toronto<br />
<a href="http://www.cs.cornell.edu/people/sengers/">Phoebe Sengers</a>, Department of Information Science and S&amp;TS, Cornell University<br />
<a href="http://terpconnect.umd.edu/~kshilton/">Katie Shilton</a>, College of Information Studies, University of Maryland<br />
<a href="http://www.natashadowschull.org/">Natasha Dow Schüll</a>, Program in Science, Technology, and Society, MIT<br />
<a href="http://socialcomputing.uci.edu/users/luke-stark">Luke Stark</a> (RA), Department of Media, Culture, and Communication, NYU<br />
<a href="http://www.michaelzimmer.org/">Michael Zimmer</a>, School of Information Studies, University of Wisconsin, Milwaukee</p>
]]></content:encoded>
							<wfw:commentRss>https://valuesindesign.wordpress.com/2019/07/30/values-in-design-council/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:thumbnail url="https://valuesindesign.files.wordpress.com/2014/06/nsf.jpg" />
		<media:content url="https://valuesindesign.files.wordpress.com/2014/06/nsf.jpg" medium="image">
			<media:title type="html">nsf</media:title>
		</media:content>

		<media:content url="https://2.gravatar.com/avatar/2fdc34cca913c2666ddbec9cde9ac632?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">emilygoldsher</media:title>
		</media:content>
	</item>
		<item>
		<title>Protected: VID Council</title>
		<link>https://valuesindesign.wordpress.com/2013/06/30/vid-council/</link>
				<comments>https://valuesindesign.wordpress.com/2013/06/30/vid-council/#respond</comments>
				<pubDate>Sun, 30 Jun 2013 22:52:38 +0000</pubDate>
		<dc:creator><![CDATA[emilygoldsher]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">http://valuesindesign.wordpress.com/?p=349</guid>
				<description><![CDATA[There is no excerpt because this is a protected post.]]></description>
								<content:encoded><![CDATA[<p>This post is password protected. You must visit the website and enter the password to continue reading.</p>
]]></content:encoded>
							<wfw:commentRss>https://valuesindesign.wordpress.com/2013/06/30/vid-council/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:content url="https://2.gravatar.com/avatar/2fdc34cca913c2666ddbec9cde9ac632?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">emilygoldsher</media:title>
		</media:content>
	</item>
	</channel>
</rss>
